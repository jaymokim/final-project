---
title: "K-Means"
author: "Seung Ah Ha, Jaymo Kim, Wonbin Song"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

Algorithm:
First randomly assign a group label from 1 to 3 to each observations.
Then calculate the current group centers and re-assign each observation to the 
cluster whose center it is the closest to(using the Euclidean distance). And we iterate until the assignment of observations to groups stops changing.


We implemented the algorithm by first creating the function "initialft" that calculates the group centers and the Euclidean distances and returns new group label
where each elements of the new group lable is the group whose center it is closest to. And we created the function "kmeans_ft" that iterates the function "initialft" to compare new group label to previous group label and returns the new group label when the new group label equals the previous group label.


```{r}

# install.packages('rattle')
data(wine, package="rattle")

initialft <- function(mat.data, n.row, n.col, G.label, new.data)
{
    # calculates the group centres
    mat.mu <- matrix(0,3,(n.col+1))
    for(j in 2:(n.col+1)){
        for(i in 1:3){
            mat.mu[i,j] <- mean(new.data[G.label==i,j])
        }
    }
    mat.mu[1,1]<- 1
    mat.mu[2,1]<- 2
    mat.mu[3,1]<- 3
    
    # calculates the Euclidean distance
    distance <- matrix(0,n.row,3)
    new.label <- rep(0,n.row)
    for(i in 1:n.row){
        for(j in 2:(n.col+1)){
            distance[i,1] <- sqrt(sum((new.data[i,j]-mat.mu[1,j])^2))
            distance[i,2] <- sqrt(sum((new.data[i,j]-mat.mu[2,j])^2))
            distance[i,3] <- sqrt(sum((new.data[i,j]-mat.mu[3,j])^2))
        }
    }
    
    # re-assign
    for (i in 1:n.row){
        new.label[i] <- which(distance[i,]==min(distance[i,]))
    }
    
    return(new.label)
}

kmeans_ft <- function(data)
{
    mat.data <- data.matrix(data)
    n.row <- nrow(mat.data)
    n.col <- ncol(mat.data)
    
    # randomly assigning a group label
    G.label <- rep(0,n.row)
    for(i in 1:n.row){
        G.label[i] <- sample(1:3,1)
    }
    new.data <- cbind(G.label,mat.data)
    
    new.l <- initialft(mat.data, n.row, n.col, G.label, new.data)
    n.iter <- 1
    while(any(new.l != G.label))
    {
        G.label <- new.l
        new.l <- initialft(mat.data, n.row, n.col, G.label, new.data)
        n.iter <- n.iter + 1
    }
    return(list(number_of_iter = n.iter, new_group = new.l))
}



# install.packages("cluster")
# install.packages("fpc")
library(cluster)
library(fpc)
cluster <- kmeans_ft(wine[-1])
plotcluster(wine[-1], cluster$new_group)

cluster
wine[,1][which(cluster$new_group==1)]
wine[,1][which(cluster$new_group==2)]
wine[,1][which(cluster$new_group==3)]

```

From the plot, We can see that the clusters seem well seperated but failed to distinguish wine type 2 from wine type 3.



```{r}

data.train <- scale(wine[-1])
cluster.scale <- kmeans_ft(data.train)
plotcluster(data.train, cluster.scale$new_group)

cluster.scale <- kmeans_ft(data.train)
wine[,1][which(cluster.scale$new_group==1)]
wine[,1][which(cluster.scale$new_group==2)]
wine[,1][which(cluster.scale$new_group==3)]
```

"scale" function calculates each column's mean and standard deviation, then scale each element by subtracting the mean and dividing by the standard deviation.

The scaled data improved the clustering results, but still failed to distinguish from wine type 2 and wine type3.


```{r}

cluster2 <- kmeans_ft(iris[,-5])
with(iris, pairs(iris[-5], col=c(1:3)[cluster2$new_group])) 

k <- kmeans_ft(iris[,1:4])
k
iris[,5][which(k$new_group==1)]
iris[,5][which(k$new_group==2)]
iris[,5][which(k$new_group==3)]
```

```{r}

cluster2.scale <- kmeans_ft(scale(iris[,1:4]))
with(iris, pairs(scale(iris[,1:4]), col=c(1:3)[cluster2.scale$new_group]))

k2 <- kmeans_ft(scale(iris[,1:4]))
k2
iris[,5][which(k2$new_group==1)]
iris[,5][which(k2$new_group==2)]
iris[,5][which(k2$new_group==3)]
```


Clusters of both scaled and unscaled data classified the three different species of Iris data almost perfectly.